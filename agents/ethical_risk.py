import os
import sys
import json
from typing import Dict, List, Any
from datetime import datetime
from pydantic import BaseModel, Field

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langgraph.graph import StateGraph, END
from utils.vector_db import VectorDBManager
from langchain_openai import ChatOpenAI

# ìƒíƒœ í´ë˜ìŠ¤ ì •ì˜
class EthicalRiskState(BaseModel):
    # ì…ë ¥
    service_info: Dict[str, Any] = Field(default_factory=dict, description="ê²€ì¦ëœ ì„œë¹„ìŠ¤ ì •ë³´")
    
    # ì¤‘ê°„ ì²˜ë¦¬ ê²°ê³¼
    guideline_summary: Dict[str, Any] = Field(default_factory=dict, description="ê°€ì´ë“œë¼ì¸ ìš”ì•½ ì •ë³´")
    risk_categories: List[str] = Field(default_factory=list, description="ë¶„ì„í•  ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬")
    
    # ì¶œë ¥
    risk_assessments: List[Dict[str, Any]] = Field(default_factory=list, description="ë¦¬ìŠ¤í¬ í‰ê°€ ê²°ê³¼")
    assessment_status: str = Field(default="", description="í‰ê°€ ìƒíƒœ (completed, failed)")
    timestamp: str = Field(default="", description="í‰ê°€ ìˆ˜í–‰ ì‹œê°„")
    error_message: str = Field(default="", description="ì˜¤ë¥˜ ë©”ì‹œì§€(ìˆëŠ” ê²½ìš°)")

# ì—ì´ì „íŠ¸ ë…¸ë“œ: GuidelineRetriever
def guideline_retriever(state: EthicalRiskState) -> EthicalRiskState:
    """
    ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ì„ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.
    """
    print("ğŸ“š ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì¤‘...")
    
    if not state.service_info:
        return EthicalRiskState(
            service_info=state.service_info,
            assessment_status="failed",
            error_message="ì„œë¹„ìŠ¤ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
    
    # VectorDB ê²€ìƒ‰
    try:
        db_manager = VectorDBManager()
        collection_name = "ethics_guidelines"
        
        # ì£¼ìš” ìœ¤ë¦¬ì  ì£¼ì œë³„ë¡œ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
        categories = ["í¸í–¥ì„±", "í”„ë¼ì´ë²„ì‹œ", "íˆ¬ëª…ì„±", "ì•ˆì „ì„±", "ì±…ì„ì„±"]
        guideline_summary = {}
        
        for category in categories:
            # ì¹´í…Œê³ ë¦¬ë³„ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰
            results = db_manager.search(
                collection_name=collection_name,
                query=f"{category} ê´€ë ¨ ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸",
                k=3,
                filter={"type": "guideline"}
            )
            
            # ê²°ê³¼ì—ì„œ ìš°ì„ ìˆœìœ„(UNESCO > OECD > ê¸°íƒ€)ë¥¼ ê³ ë ¤í•˜ì—¬ ì •ë ¬
            sorted_results = sorted(results, key=lambda x: x.metadata.get("priority", 999))
            
            category_items = []
            for result in sorted_results:
                org = result.metadata.get("organization", "ê¸°íƒ€")
                content = result.page_content
                
                # í˜ì´ì§€ ë²ˆí˜¸ì™€ ë¬¸ì„œëª… ì¶”ì¶œ
                page_num = result.metadata.get("page_number", "")
                file_name = result.metadata.get("file_name", "").replace(".pdf", "")
                
                category_items.append({
                    "content": content,
                    "source": f"{org} ({file_name}, p.{page_num})"
                })
            
            guideline_summary[category] = category_items
        
        print(f"âœ… {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ì˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì™„ë£Œ")
        
        return EthicalRiskState(
            service_info=state.service_info,
            guideline_summary=guideline_summary,
            risk_categories=categories
        )
    
    except Exception as e:
        error_message = f"ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"âŒ {error_message}")
        
        return EthicalRiskState(
            service_info=state.service_info,
            assessment_status="failed",
            error_message=error_message
        )

# ì—ì´ì „íŠ¸ ë…¸ë“œ: RiskAssessor
def risk_assessor(state: EthicalRiskState) -> EthicalRiskState:
    """
    ê°€ì´ë“œë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì„œë¹„ìŠ¤ì˜ ìœ¤ë¦¬ì  ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    """
    print("ğŸ” ìœ¤ë¦¬ì  ë¦¬ìŠ¤í¬ í‰ê°€ ì¤‘...")
    
    if not state.guideline_summary or not state.risk_categories:
        return EthicalRiskState(
            service_info=state.service_info,
            guideline_summary=state.guideline_summary,
            assessment_status="failed",
            error_message="ê°€ì´ë“œë¼ì¸ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
        )
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
    
    service_info_text = json.dumps(state.service_info, ensure_ascii=False, indent=2)
    risk_assessments = []
    
    for category in state.risk_categories:
        print(f"  - {category} ì¹´í…Œê³ ë¦¬ í‰ê°€ ì¤‘...")
        
        # ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ê°€ì´ë“œë¼ì¸ ì¶”ì¶œ
        category_guidelines = state.guideline_summary.get(category, [])
        if not category_guidelines:
            continue
            
        guidelines_text = "\n\n".join([
            f"ì¶œì²˜: {item['source']}\në‚´ìš©: {item['content']}" 
            for item in category_guidelines
        ])
        
        # LLMìœ¼ë¡œ ë¦¬ìŠ¤í¬ í‰ê°€
        assessment_prompt = f"""
        ë‹¹ì‹ ì€ AI ìœ¤ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ AI ì„œë¹„ìŠ¤ì— ëŒ€í•´ "{category}" ì¸¡ë©´ì˜ ìœ¤ë¦¬ì  ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.
        
        ## AI ì„œë¹„ìŠ¤ ì •ë³´
        ```json
        {service_info_text}
        ```
        
        ## ê´€ë ¨ ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸
        {guidelines_text}
        
        ë‹¤ìŒ êµ¬ì¡°ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: "ë†’ìŒ", "ì¤‘ê°„", "ë‚®ìŒ" ì¤‘ í•˜ë‚˜ë¡œ í‰ê°€
        2. ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸: ìµœëŒ€ 3ê°œê¹Œì§€ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ 
        3. ê·¼ê±°: ê° ë¦¬ìŠ¤í¬ ìš”ì¸ì´ ê°€ì´ë“œë¼ì¸ì˜ ì–´ë–¤ ë¶€ë¶„ì„ ìœ„ë°˜í•˜ëŠ”ì§€ ì„¤ëª…
        4. ê¸°ì¤€ ë¬¸ì„œ: íŒë‹¨ì˜ ê·¼ê±°ê°€ ëœ ì£¼ìš” ë¬¸ì„œ ì°¸ì¡°
        
        ì¶œë ¥ í˜•ì‹:
        {{
          "category": "{category}",
          "risk_level": "ë†’ìŒ|ì¤‘ê°„|ë‚®ìŒ",
          "risk_factors": [
            {{
              "name": "ë¦¬ìŠ¤í¬ ìš”ì¸ëª…",
              "description": "ìƒì„¸ ì„¤ëª…",
              "guideline_reference": "ê´€ë ¨ ê°€ì´ë“œë¼ì¸ ì¡°í•­"
            }},
            ...
          ],
          "evidence": "ì¢…í•©ì ì¸ í‰ê°€ ê·¼ê±°",
          "reference_documents": ["ì°¸ì¡° ë¬¸ì„œ1", ...]
        }}
        """
        
        try:
            response = llm.invoke(assessment_prompt)
            assessment_text = response.content
            
            # JSON í˜•ì‹ ì¶”ì¶œ
            import re
            json_match = re.search(r'\{.*\}', assessment_text, re.DOTALL)
            
            if json_match:
                assessment = json.loads(json_match.group(0))
                risk_assessments.append(assessment)
            else:
                print(f"  âš ï¸ {category} í‰ê°€ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"  âš ï¸ {category} í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    if not risk_assessments:
        return EthicalRiskState(
            service_info=state.service_info,
            guideline_summary=state.guideline_summary,
            risk_categories=state.risk_categories,
            assessment_status="failed",
            error_message="ë¦¬ìŠ¤í¬ í‰ê°€ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            timestamp=datetime.now().isoformat()
        )
    
    print(f"âœ… {len(risk_assessments)}ê°œ ì¹´í…Œê³ ë¦¬ í‰ê°€ ì™„ë£Œ")
    
    return EthicalRiskState(
        service_info=state.service_info,
        guideline_summary=state.guideline_summary,
        risk_categories=state.risk_categories,
        risk_assessments=risk_assessments,
        assessment_status="completed",
        timestamp=datetime.now().isoformat()
    )

# ê·¸ë˜í”„ êµ¬ì„±
def create_ethical_risk_agent() -> StateGraph:
    """ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ ìƒì„±"""
    workflow = StateGraph(EthicalRiskState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", guideline_retriever)
    workflow.add_node("assess", risk_assessor)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.add_edge("retrieve", "assess")
    workflow.add_edge("assess", END)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("retrieve")
    
    return workflow

# ì—ì´ì „íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
def run_ethical_risk_agent(service_info: Dict[str, Any]) -> Dict[str, Any]:
    """ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì—ì´ì „íŠ¸ ì‹œì‘")
    
    # ê·¸ë˜í”„ ìƒì„± ë° ì»´íŒŒì¼
    graph = create_ethical_risk_agent()
    app = graph.compile()
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = EthicalRiskState(service_info=service_info)
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    result = app.invoke(initial_state.dict())
    
    print(f"ìœ¤ë¦¬ ë¦¬ìŠ¤í¬ ì§„ë‹¨ ì™„ë£Œ: ìƒíƒœ = {result.assessment_status}")
    
    # ê²°ê³¼ ë°˜í™˜
    return {
        "service_info": result.service_info,
        "risk_assessments": result.risk_assessments,
        "assessment_status": result.assessment_status,
        "timestamp": result.timestamp,
        "error_message": result.error_message if result.error_message else None
    }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì„œë¹„ìŠ¤ ì •ë³´
    test_service_info = {
        "title": "AI ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤",
        "domain": "ì°½ì‘ ë„êµ¬",
        "summary": "ì‚¬ìš©ìê°€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ AIê°€ ê´€ë ¨ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
        "features": [
            {"name": "í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë³€í™˜", "description": "í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±"},
            {"name": "ì´ë¯¸ì§€ í¸ì§‘", "description": "ìƒì„±ëœ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ë³€ê²½ ë° í¸ì§‘"}
        ]
    }
    
    # ì—ì´ì „íŠ¸ ì‹¤í–‰
    result = run_ethical_risk_agent(test_service_info)
    print(json.dumps(result, ensure_ascii=False, indent=2))